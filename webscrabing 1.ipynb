{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keithgalli.github.io/web-scraping/example.html\n",
    "https://keithgalli.github.io/web-scraping/webpage.html\n",
    "webpages which used in this program "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup as bs \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the webpage information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title>HTML Example</title>\n",
      "</head>\n",
      "<body>\n",
      "<div align=\"middle\">\n",
      "<h1>HTML Webpage</h1>\n",
      "<p>Link to more interesting example: <a href=\"https://keithgalli.github.io/web-scraping/webpage.html\">keithgalli.github.io/web-scraping/webpage.html</a></p>\n",
      "</div>\n",
      "<h2>A Header</h2>\n",
      "<p><i>Some italicized text</i></p>\n",
      "<h2>Another header</h2>\n",
      "<p id=\"paragraph-id\"><b>Some bold text</b></p>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r= requests.get(\"https://keithgalli.github.io/web-scraping/example.html\")\n",
    "\n",
    "#convert to beatifullsoup opject\n",
    "soup = bs(r.content) # now this object contain all webpage content \n",
    "print(soup)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start use beatifulsoup to scraping data from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>A Header</h2>\n",
      "[<h2>A Header</h2>, <h2>Another header</h2>]\n"
     ]
    }
   ],
   "source": [
    "first_header =soup.find (\"h2\")\n",
    "all_header = soup.find_all(\"h2\")\n",
    "print(first_header)\n",
    "print(all_header)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find more than arrgment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>HTML Webpage</h1>\n",
      "[<h1>HTML Webpage</h1>, <h2>A Header</h2>, <h2>Another header</h2>]\n"
     ]
    }
   ],
   "source": [
    "first_header =soup.find ([\"h1\",\"h2\"])\n",
    "all_header =soup.find_all ([\"h1\",\"h2\"])\n",
    "print (first_header)\n",
    "print (all_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p>Link to more interesting example: <a href=\"https://keithgalli.github.io/web-scraping/webpage.html\">keithgalli.github.io/web-scraping/webpage.html</a></p>, <p><i>Some italicized text</i></p>, <p id=\"paragraph-id\"><b>Some bold text</b></p>]\n",
      "[<p id=\"paragraph-id\"><b>Some bold text</b></p>]\n"
     ]
    }
   ],
   "source": [
    "paragraph= soup.find_all(\"p\") # find all paragraph \n",
    "spesific_paragraph = soup.find_all(\"p\",attrs={\"id\":\"paragraph-id\"}) # in all paragraph find id = paragraph-id\n",
    "print (paragraph)\n",
    "print(spesific_paragraph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neast a find and find_all to find aspesific information in  the wepbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<body>\n",
      "<div align=\"middle\">\n",
      "<h1>HTML Webpage</h1>\n",
      "<p>Link to more interesting example: <a href=\"https://keithgalli.github.io/web-scraping/webpage.html\">keithgalli.github.io/web-scraping/webpage.html</a></p>\n",
      "</div>\n",
      "<h2>A Header</h2>\n",
      "<p><i>Some italicized text</i></p>\n",
      "<h2>Another header</h2>\n",
      "<p id=\"paragraph-id\"><b>Some bold text</b></p>\n",
      "</body>\n",
      "<div align=\"middle\">\n",
      "<h1>HTML Webpage</h1>\n",
      "<p>Link to more interesting example: <a href=\"https://keithgalli.github.io/web-scraping/webpage.html\">keithgalli.github.io/web-scraping/webpage.html</a></p>\n",
      "</div>\n",
      "<h1>HTML Webpage</h1>\n"
     ]
    }
   ],
   "source": [
    "body=soup.find(\"body\")\n",
    "print(body)\n",
    "#find the dive inside the body\n",
    "div=body.find(\"div\")\n",
    "print(div)\n",
    "#find the header iside the div\n",
    "header=div.find(\"h1\")\n",
    "print(header)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "search spesific string using find / find_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p id=\"paragraph-id\"><b>Some bold text</b></p>]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_string = soup.find_all(\"p\", string=\"Some bold text\")\n",
    "find_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<p>Link to more interesting example: <a href=\"https://keithgalli.github.io/web-scraping/webpage.html\">keithgalli.github.io/web-scraping/webpage.html</a></p>]\n",
      "[<p><i>Some italicized text</i></p>, <p id=\"paragraph-id\"><b>Some bold text</b></p>]\n",
      "[<b>Some bold text</b>]\n"
     ]
    }
   ],
   "source": [
    "content= soup.select(\"div p\")  # inside \"div\" select the \"p\"\n",
    "print(content)\n",
    "\n",
    "paragraph= soup.select(\"h2 ~  p\")                           # [ for ~ and # search for css selector]\n",
    "print(paragraph)\n",
    "\n",
    "bold_text= soup.select(\"p#paragraph-id  b \")\n",
    "print(bold_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different proprties of soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method PageElement.get_text of <div align=\"middle\">\n",
      "<h1>HTML Webpage</h1>\n",
      "<p>Link to more interesting example: <a href=\"https://keithgalli.github.io/web-scraping/webpage.html\">keithgalli.github.io/web-scraping/webpage.html</a></p>\n",
      "</div>>\n"
     ]
    }
   ],
   "source": [
    "# . string convert to string \n",
    "header= soup.find(\"h2\")\n",
    "header.string\n",
    "# to convert to text we use get_text  in case  there is multiple child elment   \n",
    "div=soup.find(\"div\")\n",
    "div\n",
    "\n",
    "print(div.get_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " get a spesific  property from an element "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"https://keithgalli.github.io/web-scraping/webpage.html\">keithgalli.github.io/web-scraping/webpage.html</a>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://keithgalli.github.io/web-scraping/webpage.html'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link=soup.find(\"a\")\n",
    "print (link)\n",
    "link[\"href\"] # to find only the link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HTML Webpage'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path syntax\n",
    "\n",
    "soup.body\n",
    "soup.body.div\n",
    "soup.body.div.h1.string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "working with exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   HTML Example\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <div align=\"middle\">\n",
      "   <h1>\n",
      "    HTML Webpage\n",
      "   </h1>\n",
      "   <p>\n",
      "    Link to more interesting example:\n",
      "    <a href=\"https://keithgalli.github.io/web-scraping/webpage.html\">\n",
      "     keithgalli.github.io/web-scraping/webpage.html\n",
      "    </a>\n",
      "   </p>\n",
      "  </div>\n",
      "  <h2>\n",
      "   A Header\n",
      "  </h2>\n",
      "  <p>\n",
      "   <i>\n",
      "    Some italicized text\n",
      "   </i>\n",
      "  </p>\n",
      "  <h2>\n",
      "   Another header\n",
      "  </h2>\n",
      "  <p id=\"paragraph-id\">\n",
      "   <b>\n",
      "    Some bold text\n",
      "   </b>\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r= requests.get(\"https://keithgalli.github.io/web-scraping/webpage.html\")\n",
    "\n",
    "#convert to beatifullsoup opject\n",
    "webpage= bs(r.content) # now this object contain all webpage content \n",
    "print(soup.prettify())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grap all the social media wepbage  in 3 different way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.instagram.com/keithgalli/',\n",
       " 'https://twitter.com/keithgalli',\n",
       " 'https://www.linkedin.com/in/keithgalli/',\n",
       " 'https://www.tiktok.com/@keithgalli']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = webpage.select(\"ul.socials a\")\n",
    "link\n",
    "actual_links =[link[\"href\"] for link in links]\n",
    "actual_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.instagram.com/keithgalli/',\n",
       " 'https://twitter.com/keithgalli',\n",
       " 'https://www.linkedin.com/in/keithgalli/',\n",
       " 'https://www.tiktok.com/@keithgalli']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ulist = webpage.find(\"ul\",attrs={\"class\": \"socials\"})\n",
    "links=ulist.find_all(\"a\")\n",
    "actual_links =[link[\"href\"] for link in links]\n",
    "actual_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links=webpage.select(\"li.social  a\")\n",
    "actual_links =[link[\"href\"] for link in links]\n",
    "actual_links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grab the table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "\n",
    "table = webpage.select(\"table.hockey-stats\")[0]  #[0]==> mean we need the first element only\n",
    "table\n",
    "\n",
    "columns = webpage.find(\"thead\").find_all(\"th\") # grab the table column\n",
    "columns_name = [c.string for c in columns]\n",
    "\n",
    "table_rows = table.find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "empty_list = []\n",
    "for tr in table_rows:\n",
    "    td=tr.find_all(\"td\")\n",
    "    row=[str(tr.string).strip() for tr in td]\n",
    "    empty_list.append(row)\n",
    "df= pd.DataFrame(empty_list,columns=columns_name)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grab all fun facts that use  word \"is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "facts= webpage.select(\"ul.fun-facts li \")\n",
    "\n",
    "fact_with_is =[fact.find(string=re.compile(\"is\"))for fact in facts ]\n",
    "fact_with_is =[fact for fact in fact_with_is if fact]\n",
    "fact_with_is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9041f1b3911547c9502ef9c40f4149284095faa8b5b2cdb7390244859e34deed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
